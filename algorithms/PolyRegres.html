<h1 class="code-title">Polynomial Regression</h1>  
<p class="ml-type-1">Supervised Learning</p>  
<p class="ml-type-2">Regression</p>  
<p class="code-concept">Polynomial Regression is an <span class="blue">extension of linear regression</span> that models the relationship between the independent variable(s) and the dependent variable as an <span class="blue">nth-degree polynomial</span>. Despite its non-linear behavior in terms of input features, it remains <span class="blue">linear with respect to the model coefficients</span>, which allows it to be trained using ordinary least squares.</p>  
<p class="code-concept">This technique is especially useful when data shows a <span class="blue">non-linear trend</span> that cannot be captured by a straight line. By adding polynomial terms like \( x^2, x^3, \ldots, x^n \), the model becomes flexible enough to approximate more complex patterns.</p>  
<p class="code-concept">The Polynomial Regression model transforms the features as follows:</p>  
<p class="code-concept">$$y = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \cdots + \beta_nx^n + \epsilon$$</p>  
<p class="code-concept">It increases model complexity, but also the risk of <span class="blue">overfitting</span>. As the degree of the polynomial increases, the model becomes more sensitive to fluctuations in the training data, leading to poor generalization on unseen data.</p>  
<p class="code-concept">To mitigate overfitting, Polynomial Regression can be combined with <span class="blue">regularization techniques</span> such as <span class="blue">Ridge (L2)</span>, <span class="blue">Lasso (L1)</span>, or <span class="blue">Elastic-Net</span>. These help control model complexity by penalizing large coefficients.</p>  
<p class="code-concept"><span class="blue">Feature scaling is recommended</span> (when using high-degree polynomials, or the features have very different scales), but not strictly required for OLS Polynomial Regression.</p>
<p class="code-concept"><span class="blue">Feature scaling becomes crucial for Regularized Polynomial Regression</span> (Ridge, Lasso, Elastic Net), as regularization penalizes the size of coefficients, and unscaled features with large magnitudes can disproportionately affect the penalty term, leading to biased models.</p>
<p class="code-concept">OLS Linear Regression assumes <span class="blue">Linearity</span>, <span class="blue">Multicollinearity</span>, <span class="blue">Independence of Errors</span>, <span class="blue">Homoscedasticity</span>, and <span class="blue">Normality of Errors</span></p>
<p class="code-concept">In Polynomial Regression, the model assumes linearity in the transformed features, not in the original ones. Polynomial terms are inherently correlated, making multicollinearity a significant issue, adressed by employing regularization. Independence of Errors and Homoscedasticity apply unchanged, as they are fundamental to the least squares framework. Normality of Errors is relevant mainly for inference</p>
<div class="just-code">
    <!--///////////////////-->
    <button class="copy-btn">
        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#1D4ED8" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon">
            <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
            <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
        </svg>
    </button>
    <!--///////////////////-->
    <pre><code class="language-python" id="PolyRegres-code"></code></pre>
</div>
<h1 class="code-example" id="PolyRegres-project">View Example Project</h1>
