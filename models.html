<!DOCTYPE html>
<html lang="en">
<head>
    <!--Initial Specifications-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="nofollow">
    <meta http-equiv="pragma" content="no-cache">
    <!--Website Details-->
    <meta name="description" content="DataX">
    <meta http-equiv="author" content="https://github.com/FredArgoX">
    <!--Title-->
    <title>DataX</title>
    <!--Favicon-->
    <link rel="icon" type="image/png" href="./assets/logo_FAX.png">
    <!--Google Fonts-->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inclusive+Sans:ital,wght@0,300..700;1,300..700&family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap" rel="stylesheet">
    <!--Social Icons-->
    <link href="https://cdn.jsdelivr.net/npm/remixicon@4.3.0/fonts/remixicon.css" rel="stylesheet"/>
    <!--Code Snippets-->
    <!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css">-->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <!--External Styling-->
    <link rel="stylesheet" href="./style/style.css">
    <link rel="stylesheet" href="./style/code.css">
</head>
<body>
    <!-- /////////////////////////////////////////////////////// -->
    <header>
        <img src="./assets/logo_FAX.png" alt="Logo Fredy-Arce-Gonzalez" id="header-logo">
        <!--<div id="menu-container"></div>-->
    </header>
    <!-- /////////////////////////////////////////////////////// -->
    <main>
        <!-- ******************************************************* -->
        <!-- ******************************************************* -->
        <!-- ******************************************************* -->

        <div class="code-container">
            <h1 class="code-title">K-Nearest Neighbors <span class="blue">(</span>KNN<span class="blue">)</span></h1>
            <p class="ml-type-1">Supervised Learning</p>
            <p class="ml-type-2">Classification</p>
            <p class="code-concept">K-NN is a classification algorithm that assigns a label to a specific data point based on the labels of its k-nearest neighbors. Here, 'k' represents the number of neighbors considered.</p>
            <p class="code-concept">KNN is a distance-based algorithm, with "Euclidean distance" being the most commonly used. When the dataset contains features with different scales, those with larger numerical values will dominate the distance calculations. <span class="blue">Feature scaling ensures that all features contribute equally to the model.</span></p>
            <p class="code-concept"><span class="blue">Data must be scaled after splitting it into training and testing sets.</span> If scaling is done before the split, the normalization parameters (like the mean and standard deviation) are calculated from all the data, including the test set. This allows the model to indirectly gain information from the test set during training, which causes data leakage.</p>
            <p class="code-concept">Data leakage occurs when information from outside the training dataset is used to create the model. This can lead to overly optimistic results and a model that doesn’t generalize well when deployed in the real world. In the case of normalization, data leakage happens when the test set influences the training process.</p>
            <div class="just-code">
                <!--///////////////////-->
                <button class="copy-btn">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#1D4ED8" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon">
                        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
                        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
                    </svg>
                </button>
                <!--///////////////////-->
                <pre><code class="language-python" id="knn-code"></code></pre>
            </div>
            <h1 class="code-example" id="project-knn">View Example Project</h1>
        </div>
        <!-- ******************************************************* -->
        <br>
        <!-- ******************************************************* -->
        <div class="code-container">
            <h1 class="code-title">Naïve Bayes <span class="blue">(</span>NB<span class="blue">)</span></h1>
            <p class="ml-type-1">Supervised Learning</p>
            <p class="ml-type-2">Classification</p>
            <p class="code-concept">Naïve Bayes is a probabilistic classification algorithm based on Bayes' Theorem. It calculates the posterior probability of a class given the observed features and classifies data points by choosing the class with the highest probability.</p>
            <p class="code-concept">It assumes that the features in the dataset are independent of each other (this is the "naïve" assumption). It also assumes that all features contribute equally to the outcome.</p>
            <p class="code-concept">Unlike many machine learning algorithms (such as K-Nearest Neighbors, Logistic Regression, or Support Vector Machines), <span class="blue">Naïve Bayes does not require feature scaling</span> because it is based on probability calculations from the data's distribution rather than distance-based metrics.</p>
            <p class="code-concept">If using <span class="blue">GaussianNB</span>, categorical variables need to be converted to numerical form (encoding like one-hot encoding or label encoding) and standardization may improve numerical stability if feature magnitudes vary significantly, but other NB methods can directly work with categorical data, <span class="blue">CategoricalNB</span> (best for categorical features), <span class="blue">BernoulliNB</span> (for binary categorical features), <span class="blue">MultinomialNB</span> (for count-based categorical data).</p>
            <div class="just-code">
                <!--///////////////////-->
                <button class="copy-btn">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#1D4ED8" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon">
                        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
                        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
                    </svg>
                </button>
                <!--///////////////////-->
                <pre><code class="language-python" id="naiveBayes-code"></code></pre>
            </div>
            <h1 class="code-example" id="project-naiveBayes">View Example Project</h1>
        </div>
        <!-- ******************************************************* -->
        <br>
        <!-- ******************************************************* -->
        <div class="code-container">
            <h1 class="code-title">Logistic Regression</h1>
            <p class="ml-type-1">Supervised Learning</p>
            <p class="ml-type-2">Classification</p>
            <p class="code-concept">Logistic regression is a statistical method for <span class="blue">binary classification</span> that predicts the probability of an event using a logit function. It is estimated through maximum likelihood estimation (MLE) rather than ordinary least squares (OLS), which is used in linear regression.</p>
            <p class="code-concept">The sigmoid function maps values to a range between 0 and 1, making it useful for classification tasks.</p>
            <p class="code-concept"><span class="blue">Feature scaling is not required for Logistic Regression</span> like it is for distance-based algorithms. However, while it does not strictly require feature scaling, scaling can still improve performance in certain cases.</p>
            <p class="code-concept">While logistic regression is inherently a two-class classifier, techniques like one-vs-rest enable its use in multi-class problems. It can be categorized into binary logistic regression for two-class outcomes, multinomial logistic regression for multiple nominal categories, and ordinal logistic regression for ordered categories.</p>
            <div class="just-code">
                <!--///////////////////-->
                <button class="copy-btn">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#1D4ED8" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon">
                        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
                        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
                    </svg>
                </button>
                <!--///////////////////-->
                <pre><code class="language-python" id="logisticRegression-code"></code></pre>
            </div>
            <h1 class="code-example" id="project-logisticRegression">View Example Project</h1>
        </div>
        <!-- ******************************************************* -->
        <br>
        <!-- ******************************************************* -->
        <div class="code-container">
            <h1 class="code-title">Support Vector Classification <span class="blue">(</span>SVC<span class="blue">)</span></h1>
            <p class="ml-type-1">Supervised Learning</p>
            <p class="ml-type-2">Classification</p>
            <p class="code-concept">Support vector machines (SVMs) are a set of supervised learning methods used for classification and regression.</p>
            <p class="code-concept"><span class="blue">The method used for classification is called Support Vector Classification (SVC).</span></p>
            <p class="code-concept">SVC is inherently a binary classifier. It can handle multi-class classification by using strategies like One-vs-One (OvO) and One-vs-All (OvA). If desired, OvO or OvA can be implemented manually, but if not explicitly implemented, Scikit-Learn automatically handles it.</p>
            <p class="code-concept">SVC works by finding the optimal hyperplane, also known as <span class="blue">MMH</span> (Maximum Marginal Hyperplane), that best separates different classes in the dataset. The optimal hyperplane (decision boundary) maximizes the <span class="blue">margin</span> between different classes in the dataset. The margin is the distance between the hyperplane and the nearest data points from each class, which are called <span class="blue">support vectors</span>. These support vectors are critical because they define the position and orientation of the hyperplane.</p>
            <p class="code-concept">The hyperplane in 2D is a line, in 3D is a plane, and so on in higher dimensions. Some problems can’t be solved using a linear hyperplane. In such situations, SVC uses a <span class="blue">kernel trick</span> to transform the input space into a higher-dimensional space.</p>
            <p class="code-concept">SVC works naturally with numerical data since the algorithm depends on distance calculations between data points. Therefore, <span class="blue">categorical data must be transformed into numerical representations</span>.</p>
            <p class="code-concept"><span class="blue">Feature scaling is crucial for SVC</span> because the algorithm is sensitive to the scale of the input features. Scaling ensures that features contribute equally, leading to a more meaningful decision boundary.</p>
            <div class="just-code">
                <!--///////////////////-->
                <button class="copy-btn">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#1D4ED8" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon">
                        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
                        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
                    </svg>
                </button>
                <!--///////////////////-->
                <pre><code class="language-python" id="svc-code"></code></pre>
            </div>
            <h1 class="code-example" id="project-svc">View Example Project</h1>
        </div>
        <!-- ******************************************************* -->
         <br>
        <!-- ******************************************************* -->
        <div class="code-container">
            <h1 class="code-title">Support Vector Regression <span class="blue">(</span>SVR<span class="blue">)</span></h1>
            <p class="ml-type-1">Supervised Learning</p>
            <p class="ml-type-2">Regression</p>
            <p class="code-concept">Support vector machines (SVMs) are a set of supervised learning methods used for classification and regression.</p>
            <p class="code-concept"><span class="blue">The method used for regression is called Support Vector Regression (SVR).</span></p>
            <p class="code-concept">As with classification, the fit method will take as argument vectors X, y, only that in this case <span class="blue">y is expected to have floating point values</span> instead of integer values.</p>
            <p class="code-concept">Instead of maximizing the margin between classes, as performed in SVC, SVR tries to fit as many data points as possible within a given margin (<span class="blue">ε-insensitive-tube</span>) It is called -insensitive- because  predictions within this margin (±ε) are not penalized in the loss function, in other words, the model is insensitive to data points that are within the ε-range. Therefore, SVR is only sensitive to outliers because it only considers data points outside the ε margin as support vectors.</p>
            <p class="code-concept"><span class="blue">SVR supports linear and non-linear regression</span> using different kernel functions.</p>
            <p class="code-concept">Like SVM for classification, <span class="blue">feature scaling is crucial for SVR</span> because the algorithm is sensitive to distances between data points. Scaling ensures that no feature dominates the optimization process.</p>
            <div class="just-code">
                <!--///////////////////-->
                <button class="copy-btn">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#1D4ED8" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon">
                        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
                        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
                    </svg>
                </button>
                <!--///////////////////-->
                <pre><code class="language-python" id="svr-code"></code></pre>
            </div>
            <h1 class="code-example" id="project-svr">View Example Project</h1>
        </div>
        <!-- ******************************************************* -->
        <br>
        <!-- ******************************************************* -->
        <div class="code-container">
            <h1 class="code-title">Decision Tree Classification</h1>
            <p class="ml-type-1">Supervised Learning</p>
            <p class="ml-type-2">Classification</p>
            <p class="code-concept">A <span class="blue">Decision Tree (DT)</span> is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks.</p>
            <p class="code-concept">It is a flowchart-like tree structure where an internal node represents a feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome.</p>
            <p class="code-concept">Can be considered a <span class="blue">white box algorithm</span>. It shares internal decision-making logic, which is not available in the black box type of algorithms such as with a neural network.</p>
            <p class="code-concept"><span class="blue">DecisionTreeClassifier</span> is a class capable of performing multi-class classification on a dataset.</p>
            <p class="code-concept">Decision Trees <span class="blue">can handle categorical features, but they need to be properly encoded</span>. Some approaches include Label Encoding, One-Hot Encoding, Ordinal Encoding.</p>
            <p class="code-concept">When dealing with very high-dimensional datasets, it's <span class="blue">important to consider performing dimensionality reduction</span> (PCA, ICA) <span class="blue">or Feature Selection</span> beforehand to give the tree a better chance of finding features that are discriminative.</p>
            <p class="code-concept">Decision Trees partition the feature space by selecting threshold values (e.g., Age < 30) rather than computing distances. Therefore, <span class="blue">feature scaling is not required</span>.</p>
            <div class="just-code">
                <!--///////////////////-->
                <button class="copy-btn">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#1D4ED8" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon">
                        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
                        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
                    </svg>
                </button>
                <!--///////////////////-->
                <pre><code class="language-python" id="dtc-code"></code></pre>
            </div>
            <h1 class="code-example" id="project-dtc">View Example Project</h1>
        </div>
        <!-- ******************************************************* -->
        <br>
        <!-- ******************************************************* -->
        <div class="code-container">
            <h1 class="code-title">Decision Tree Regression</h1>
            <p class="ml-type-1">Supervised Learning</p>
            <p class="ml-type-2">Regression</p>
            <p class="code-concept">A <span class="blue">Decision Tree (DT)</span> is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks.</p>
            <p class="code-concept">In Regression, the goal is to predict continuous values instead of discrete class labels. It works by recursively splitting the dataset into partitions based on feature values, where each internal node represents a decision rule, and leaf nodes represent predicted numerical outputs, commonly the mean (sometimes the median) value for each partitioned region.</p>
            <p class="code-concept">Decision Trees are considered <span class="blue">white-box</span> models, meaning their decision-making process is transparent and interpretable, unlike black-box models such as neural networks.</p>
            <p class="code-concept"><span class="blue">DecisionTreeRegressor</span> is a class capable of performing regression tasks using a decision tree model.</p>
            <p class="code-concept">Decision Trees <span class="blue">can handle categorical features, but they need to be properly encoded</span>. Some approaches include Label Encoding, One-Hot Encoding, Ordinal Encoding.</p>
            <p class="code-concept">When dealing with very high-dimensional datasets, it's <span class="blue">important to consider performing dimensionality reduction</span> (PCA, ICA) <span class="blue">or Feature Selection</span> beforehand to give the tree a better chance of finding features that are discriminative.</p>
            <p class="code-concept">Decision Trees partition the feature space by selecting threshold values (e.g., Age < 30) rather than computing distances. Therefore, <span class="blue">feature scaling is not required</span>.</p>
            <div class="just-code">
                <!--///////////////////-->
                <button class="copy-btn">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#1D4ED8" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon">
                        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
                        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
                    </svg>
                </button>
                <!--///////////////////-->
                <pre><code class="language-python" id="dtr-code"></code></pre>
            </div>
            <h1 class="code-example" id="project-dtr">View Example Project</h1>
        </div>
        <!-- ******************************************************* -->
        <br>
        <!-- ******************************************************* -->
        <div class="code-container">
            <h1 class="code-title">Linear Regression</h1>
            <p class="ml-type-1">Supervised Learning</p>
            <p class="ml-type-2">Regression</p>
            <p class="code-concept">Linear Regression is a parametric supervised learning algorithm used for predicting continuous values. It models the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation to the observed data.</p>
            <p class="code-concept">The most common way to do linear regression is through ordinary least squares (<span class="blue">OLS</span>) estimation. </p>
            <p class="code-concept">OLS works by minimizing the sum of the squared differences between the observed values (the actual data points) and the predicted values from the regression line. These differences are called <span class="blue">residuals</span>, and squaring them ensures that both positive and negative residuals are treated equally. </p>
            <p class="code-concept">The Linear Regression model assumes <span class="blue">linearity</span> between the independent and dependent variables, <span class="blue">independence of errors</span> (no patterns or correlation between the residuals), <span class="blue">homoscedasticity</span> (constant variance of residuals), and <span class="blue">normality of residuals</span> (residuals should ideally follow a normal or gaussian distribution).</p>
            <p class="code-concept">Performing linear regression with categorical features requires some preprocessing since linear regression works with numerical data. <span class="blue">Categorical features need to be converted into a numerical format</span> that the model can interpret.</p>
            <p class="code-concept">For OLS linear regression without regularization, <span class="blue">there is no technical requirement for feature scaling</span>, as the coefficients will adjust to the scale of the features.</p>
            <div class="just-code">
                <!--///////////////////-->
                <button class="copy-btn">
                    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#1D4ED8" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon">
                        <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
                        <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
                    </svg>
                </button>
                <!--///////////////////-->
                <pre><code class="language-python" id="linearRegression-code"></code></pre>
            </div>
            <h1 class="code-example" id="project-linearRegression">View Example Project</h1>
        </div>
        <!-- ******************************************************* -->
       
        <!-- ******************************************************* -->
    </main>
        <!-- ******************************************************* -->
        <!-- ******************************************************* -->
        <!-- ******************************************************* -->
    <!-- /////////////////////////////////////////////////////// -->
    <footer>
        <!--<p class="author">2024 | Designed by <span class="author-tag">fargox-dev</span></p>-->
        <div class="social-container">
            <span class="github"><i class="ri-github-fill ri-2x social-icon"></i></span>
            <span class="linkedin"><i class="ri-linkedin-fill ri-2x social-icon"></i></span>
            <span class="x"><i class="ri-twitter-x-fill ri-2x social-icon"></i></span>
        </div>
    </footer>
    <!-- /////////////////////////////////////////////////////// -->
    <script src="./js/machineLearning.js"></script>
    <!-- /////////////////////////////////////////////////////// -->
</body>
</html>
